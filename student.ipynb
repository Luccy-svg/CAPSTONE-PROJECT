{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffe79e0-b375-430c-af46-bf3e1d7ec47a",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n",
    "## 1.1 Background\n",
    "\n",
    "Semiconductor manufacturing is a highly complex and capital-intensive process involving hundreds of fabrication steps that must be performed with extreme precision. Even microscopic defects introduced during wafer processing can lead to complete product failure, reducing manufacturing yield and increasing production costs.\n",
    "Traditionally, quality control in semiconductor fabrication has relied on manual inspection and rule-based systems, which are time-consuming, subjective, and often unable to keep up with modern production speeds.\n",
    "\n",
    "In recent years, semiconductor companies such as Intel, TSMC, and Samsung,Nvidia have shifted toward AI-driven defect detection systems to improve yield prediction, defect localization, and root-cause analysis. Leveraging machine learning and computer vision, these systems can detect defect patterns directly from wafer map images, enabling earlier and more accurate interventions in the production line.\n",
    "\n",
    "## 1.2 Problem Statement\n",
    "\n",
    "Manufacturers need an efficient and automated method to identify and classify wafer defects early in the production process. Manual inspection systems fail to scale with high-volume production and cannot accurately identify subtle, complex defect patterns.\n",
    "Therefore, the goal is to develop a machine learning-based image analysis model capable of automatically detecting and classifying defect patterns in wafer maps therefore improving yield, reducing inspection time, and minimizing production losses.\n",
    "\n",
    "## 1.3 Business Objective\n",
    "\n",
    "The primary business objective is to enhance production efficiency and quality assurance in semiconductor manufacturing by automating defect detection.\n",
    "The system will:\n",
    "\n",
    "- Identify wafer defect types using image-based pattern recognition.\n",
    "\n",
    "- Support process engineers in diagnosing the root cause of production faults.\n",
    "\n",
    "- Reduce manual inspection time and related operational costs.\n",
    "\n",
    "- Improve yield rate and product reliability.\n",
    "\n",
    "Ultimately, the project aims to demonstrate how AI-based defect detection can improve decision-making, reduce downtime, and ensure data-driven manufacturing optimization.\n",
    "\n",
    "## 1.4 Project Goal\n",
    "\n",
    "To build and deploy a deep learning-based image classification model capable of identifying common wafer defect patterns (e.g., center, edge-ring, scratch, random) using the WM811K dataset. The modelâ€™s predictions will be integrated into an interactive Streamlit dashboard, allowing users to:\n",
    "\n",
    "- Upload wafer map images,\n",
    "- View real-time defect classification and confidence levels, and\n",
    "- Visualize feature importance or activation maps (Grad-CAM) for interpretability.\n",
    "\n",
    "## 1.5 Expected Business Impact\n",
    "\n",
    "- `Operational Efficiency:`\tFaster and more accurate defect detection compared to manual methods.\n",
    "- `Cost Reduction:`\tReduced labor costs and fewer defective chips reaching final testing.\n",
    "- `Quality Improvement:` Early detection minimizes yield loss and improves product reliability.\n",
    "- `Decision Support:`\tData-driven insights for process optimization and predictive maintenance.\n",
    "- `Scalability:`\tSystem can be integrated into production pipelines and scaled to new wafer types.\n",
    "## 1.6 Success Metrics\n",
    "\n",
    "- Accuracy / F1 Score of classification model \n",
    "\n",
    "- Reduction in defect inspection time by .\n",
    "\n",
    "- Improved detection of rare defect patterns (using confusion matrix or recall metrics).\n",
    "\n",
    "- Usability feedback from engineers or end-users on the Streamlit dashboard prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e692c523-6bb8-4d42-944b-929a0358dbae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'WM811K.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWM811K.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda\\Lib\\site-packages\\pandas\\io\\pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    186\u001b[0m     filepath_or_buffer,\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    188\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    189\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    190\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    191\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'WM811K.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"WM811K.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a1129-45ad-4bb0-8469-e754c0d7580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df, target_samples_per_class=800):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset with strategic sampling\n",
    "    \"\"\"\n",
    "    print(\"=== PHASE 3: DATA PREPARATION ===\")\n",
    "    \n",
    "    # 1. Remove [0 0] class as requested\n",
    "    print(\"1. Removing '[0 0]' class...\")\n",
    "    df = df[df['failureType'] != '[0 0]']\n",
    "    \n",
    "    # 2. Analyze original distribution\n",
    "    print(\"\\n2. Original Class Distribution:\")\n",
    "    original_dist = df['failureType'].value_counts()\n",
    "    for defect_type, count in original_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {defect_type:12}: {count:5} samples ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # 3. Create balanced dataset\n",
    "    print(f\"\\n3. Creating balanced dataset with target of {target_samples_per_class} samples per class\")\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    \n",
    "    for class_name in df['failureType'].unique():\n",
    "        class_df = df[df['failureType'] == class_name]\n",
    "        \n",
    "        if len(class_df) >= target_samples_per_class:\n",
    "            # If we have enough, take target amount\n",
    "            sampled_df = class_df.sample(n=target_samples_per_class, random_state=42)\n",
    "        else:\n",
    "            # For rare classes, use all available\n",
    "            sampled_df = class_df.copy()\n",
    "            print(f\"  Using all {len(sampled_df)} samples for rare class: {class_name}\")\n",
    "        \n",
    "        balanced_dfs.append(sampled_df)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42)\n",
    "    \n",
    "    print(f\"\\n4. Final Balanced Dataset:\")\n",
    "    print(f\"   Total samples: {len(df_balanced)}\")\n",
    "    print(f\"   Number of classes: {df_balanced['failureType'].nunique()}\")\n",
    "    \n",
    "    # Verify balance\n",
    "    print(\"\\n5. Balanced Class Distribution:\")\n",
    "    balanced_dist = df_balanced['failureType'].value_counts()\n",
    "    for defect_type, count in balanced_dist.items():\n",
    "        percentage = (count / len(df_balanced)) * 100\n",
    "        print(f\"  {defect_type:12}: {count:4} samples ({percentage:5.1f}%)\")\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "# Create balanced dataset\n",
    "df_balanced = create_balanced_dataset(df, target_samples_per_class=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150434c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_advanced_features(df):\n",
    "    \"\"\"\n",
    "    Extract comprehensive features from wafer maps\n",
    "    \"\"\"\n",
    "    print(\"\\n6. Feature Engineering...\")\n",
    "    \n",
    "    def extract_wafer_features(wafer_map):\n",
    "        if isinstance(wafer_map, np.ndarray):\n",
    "            # Handle different data types\n",
    "            if wafer_map.dtype == bool:\n",
    "                wafer_map = wafer_map.astype(int)\n",
    "            \n",
    "            flat_map = wafer_map.flatten()\n",
    "            non_zero_pixels = flat_map[flat_map > 0]\n",
    "            \n",
    "            features = {\n",
    "                # Basic intensity features\n",
    "                'mean_intensity': np.mean(wafer_map),\n",
    "                'std_intensity': np.std(wafer_map),\n",
    "                'defect_density': np.sum(wafer_map > 0) / wafer_map.size if wafer_map.size > 0 else 0,\n",
    "                'max_intensity': np.max(wafer_map) if len(wafer_map) > 0 else 0,\n",
    "                'min_intensity': np.min(wafer_map) if len(wafer_map) > 0 else 0,\n",
    "                \n",
    "                # Statistical features\n",
    "                'defect_variance': np.var(non_zero_pixels) if len(non_zero_pixels) > 0 else 0,\n",
    "                'defect_skewness': pd.Series(non_zero_pixels).skew() if len(non_zero_pixels) > 0 else 0,\n",
    "                \n",
    "                # Spatial distribution features\n",
    "                'center_defect_ratio': calculate_center_defect_ratio(wafer_map),\n",
    "                'edge_defect_ratio': calculate_edge_defect_ratio(wafer_map),\n",
    "                'corner_defect_ratio': calculate_corner_defect_ratio(wafer_map),\n",
    "                \n",
    "                # Shape features\n",
    "                'aspect_ratio': calculate_aspect_ratio(wafer_map),\n",
    "                'symmetry_score': calculate_symmetry(wafer_map)\n",
    "            }\n",
    "            return features\n",
    "        return {}\n",
    "    \n",
    "    def calculate_center_defect_ratio(wafer_map):\n",
    "        \"\"\"Calculate ratio of defects in center region\"\"\"\n",
    "        h, w = wafer_map.shape\n",
    "        center_region = wafer_map[h//4:3*h//4, w//4:3*w//4]\n",
    "        total_defects = np.sum(wafer_map > 0)\n",
    "        return np.sum(center_region > 0) / total_defects if total_defects > 0 else 0\n",
    "    \n",
    "    def calculate_edge_defect_ratio(wafer_map):\n",
    "        \"\"\"Calculate ratio of defects in edge region\"\"\"\n",
    "        h, w = wafer_map.shape\n",
    "        edge_region = np.copy(wafer_map)\n",
    "        edge_region[h//4:3*h//4, w//4:3*w//4] = 0\n",
    "        total_defects = np.sum(wafer_map > 0)\n",
    "        return np.sum(edge_region > 0) / total_defects if total_defects > 0 else 0\n",
    "    \n",
    "    def calculate_corner_defect_ratio(wafer_map):\n",
    "        \"\"\"Calculate ratio of defects in corner regions\"\"\"\n",
    "        h, w = wafer_map.shape\n",
    "        corner_size = min(h, w) // 3\n",
    "        corners = (np.sum(wafer_map[:corner_size, :corner_size] > 0) +\n",
    "                  np.sum(wafer_map[:corner_size, -corner_size:] > 0) +\n",
    "                  np.sum(wafer_map[-corner_size:, :corner_size] > 0) +\n",
    "                  np.sum(wafer_map[-corner_size:, -corner_size:] > 0))\n",
    "        total_defects = np.sum(wafer_map > 0)\n",
    "        return corners / total_defects if total_defects > 0 else 0\n",
    "    \n",
    "    def calculate_aspect_ratio(wafer_map):\n",
    "        \"\"\"Calculate aspect ratio of defect pattern\"\"\"\n",
    "        defect_positions = np.argwhere(wafer_map > 0)\n",
    "        if len(defect_positions) == 0:\n",
    "            return 1.0\n",
    "        min_y, min_x = defect_positions.min(axis=0)\n",
    "        max_y, max_x = defect_positions.max(axis=0)\n",
    "        height = max_y - min_y + 1\n",
    "        width = max_x - min_x + 1\n",
    "        return width / height if height > 0 else 1.0\n",
    "    \n",
    "    def calculate_symmetry(wafer_map):\n",
    "        \"\"\"Calculate symmetry score\"\"\"\n",
    "        if wafer_map.shape[0] != wafer_map.shape[1]:\n",
    "            return 0.5\n",
    "        horizontal_symmetry = np.sum(wafer_map == np.fliplr(wafer_map)) / wafer_map.size\n",
    "        vertical_symmetry = np.sum(wafer_map == np.flipud(wafer_map)) / wafer_map.size\n",
    "        return (horizontal_symmetry + vertical_symmetry) / 2\n",
    "    \n",
    "    # Apply feature extraction\n",
    "    print(\"Extracting features from wafer maps...\")\n",
    "    wafer_features = df['waferMap'].apply(extract_wafer_features)\n",
    "    wafer_features_df = pd.DataFrame(wafer_features.tolist())\n",
    "    \n",
    "    # Combine with original features\n",
    "    df_processed = pd.concat([df[['dieSize', 'failureType']], wafer_features_df], axis=1)\n",
    "    df_processed = df_processed.dropna()\n",
    "    \n",
    "    print(f\"Final processed dataset: {df_processed.shape}\")\n",
    "    print(f\"Features: {df_processed.columns.tolist()}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Extract features\n",
    "df_processed = extract_advanced_features(df_balanced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
